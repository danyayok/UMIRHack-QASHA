import logging
import asyncio
import g4f
import aiohttp
import json

import requests
from gigachat import GigaChat
import os
import time
from app.core.config import settings
from typing import Optional, Dict
import ollama
from ollama import Client


logger = logging.getLogger("qa_automata")


class HybridAIService:
    def __init__(self):
        self.giga = None
        self.giga_available = False
        self.ollama_available = False
        self.ollama_model = getattr(settings, 'OLLAMA_MODEL', 'qwen3-coder:480b')
        self._init_gigachat()
        self._init_ollama()

    def _init_gigachat(self):
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ GigaChat ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ API ĞºĞ»ÑÑ‡"""
        try:
            giga_key = settings.GIGACHAT_KEY
            if giga_key:
                self.giga = GigaChat(
                    credentials=giga_key,
                    verify_ssl_certs=False,
                    model="GigaChat"
                )
                self.giga_available = True
                logger.info("GigaChat initialized successfully")
            else:
                logger.info("GIGACHAT_KEY not found, GigaChat will not be available")
        except Exception as e:
            logger.error(f"Failed to initialize GigaChat: {e}")
            self.giga_available = False

    def _init_ollama(self):
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ollama"""
        try:
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ API
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {settings.OLLAMA_API_KEY}'
            }

            response = requests.get(
                f"{settings.OLLAMA_HOST}/api/tags",
                headers=headers,
                timeout=10
            )

            if response.status_code == 200:
                self.ollama_available = True
                logger.info(f"âœ… Ollama cloud initialized successfully with model: {self.ollama_model}")
            else:
                logger.error(f"âŒ Ollama initialization failed: {response.status_code} - {response.text}")
                self.ollama_available = False

        except Exception as e:
            logger.error(f"Failed to initialize Ollama cloud: {e}")
            self.ollama_available = False

    async def answer_with_ollama(self, text: str, prompt: str, timeout: int = 120) -> Optional[str]:
        """Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ollama Ñ Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚Ğ¾Ğ¼"""
        if not self.ollama_available:
            logger.info("Ollama not available")
            return None

        try:
            response = await asyncio.wait_for(
                asyncio.to_thread(self._sync_ollama_request, text, prompt),
                timeout=timeout
            )

            if response and self._validate_ai_response(response):
                logger.info(f"âœ… Ollama response received, length: {len(response)}")
                return response
            else:
                logger.warning(f"Ollama response invalid: {response}")
                return None

        except asyncio.TimeoutError:
            logger.error(f"Ollama request timed out after {timeout} seconds")
            return None
        except Exception as e:
            logger.error(f"Ollama error: {e}")
            return None

    def _sync_ollama_request(self, text: str, prompt: str) -> Optional[str]:
        """Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğº Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ollama"""
        try:
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚
            full_prompt = f"{prompt}\n\nĞ—Ğ°Ğ¿Ñ€Ğ¾Ñ: {text}"

            payload = {
                "model": self.ollama_model,
                "prompt": full_prompt,
                "stream": False
            }

            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {settings.OLLAMA_API_KEY}'
            }

            response = requests.post(
                f"{settings.OLLAMA_HOST}/api/generate",
                headers=headers,
                json=payload,
                timeout=120
            )

            if response.status_code == 200:
                data = response.json()
                return data.get('response', '').strip()
            else:
                logger.error(f"Ollama API error: {response.status_code} - {response.text}")
                return None

        except Exception as e:
            logger.error(f"Ollama cloud request failed: {e}")
            return None

    async def answer_with_g4f(self, text: str, prompt: str, model: str = 'gpt-4', timeout: int = 60) -> Optional[str]:
        """Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº g4f Ñ Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚Ğ¾Ğ¼"""
        try:
            task = asyncio.create_task(self._g4f_request(text, prompt, model))
            response = await asyncio.wait_for(task, timeout=timeout)

            if response and "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ñ Ğ½Ğµ Ğ¼Ğ¾Ğ³Ñƒ" not in response and len(response) > 30:
                return response
            else:
                logger.warning(f"g4f response too short or contains refusal: {response}")
                return None

        except asyncio.TimeoutError:
            logger.error(f"g4f request timed out after {timeout} seconds")
            return None
        except Exception as e:
            logger.error(f"g4f error: {e}")
            return None

    async def _g4f_request(self, text: str, prompt: str, model: str) -> Optional[str]:
        """Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğº g4f"""
        try:
            response = await g4f.ChatCompletion.create_async(
                model=model,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": text}
                ],
                timeout=60
            )
            return response
        except Exception as e:
            logger.error(f"g4f request failed: {e}")
            return None

    async def answer_with_gigachat(self, text: str, prompt: str, timeout: int = 30) -> Optional[str]:
        if not self.giga_available or not self.giga:
            return None

        try:
            full_prompt = f"{prompt}\n\nĞ—Ğ°Ğ¿Ñ€Ğ¾Ñ: {text}"
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.giga.chat(full_prompt)
            )
            return response.choices[0].message.content
        except asyncio.TimeoutError:
            logger.error("GigaChat request timed out")
            return None
        except Exception as e:
            logger.error(f"GigaChat error: {e}")
            return None

    async def generate_test_content(self, file_info: Dict, project_context: Dict,
                                    test_type: str, framework: str, config: Dict) -> Optional[str]:
        """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ‚ĞµÑÑ‚Ğ° Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""

        if not isinstance(file_info, dict):
            logger.error(f"AI_VALIDATION_ERROR: file_info must be dict, got {type(file_info)}")
            if isinstance(file_info, list) and file_info:
                file_info = file_info[0] if isinstance(file_info[0], dict) else {"path": "unknown"}
            else:
                file_info = {"path": "unknown", "name": "unknown", "type": "unknown"}

        if not isinstance(project_context, dict):
            logger.error(f"AI_VALIDATION_ERROR: project_context must be dict, got {type(project_context)}")
            project_context = {}

        required_fields = ['path', 'name', 'has_content', 'ignored']
        for field in required_fields:
            if field not in file_info:
                file_info[field] = field == 'ignored'

        logger.info(f"AI_RECEIVED_DATA:")
        logger.info(f"  - File: {file_info.get('path', 'unknown')}")
        logger.info(f"  - File type: {type(file_info)}")
        logger.info(f"  - File keys: {list(file_info.keys())}")
        logger.info(f"  - Project technologies: {project_context.get('project_metadata', {}).get('technologies', [])}")
        logger.info(f"  - Has content: {file_info.get('has_content', False)}")

        if not project_context.get('project_metadata', {}).get('technologies'):
            logger.warning("AI: No technologies in project context!")

        if not file_info.get('has_content', False):
            logger.warning("AI: No file content available!")

        try:
            logger.info(f"AI_START: Generating {test_type} test for {file_info.get('path', 'unknown')}")
            logger.info(f"AI_INFO: Framework: {framework}, File type: {file_info.get('type', 'unknown')}")

            prompt = self._create_test_generation_prompt(test_type, framework, config)
            request_data = self._prepare_test_request_data(file_info, project_context, test_type, framework, config)

            logger.info(f"AI_PROMPT: Prompt length: {len(prompt)} chars")
            logger.info(f"AI_DATA: Request data length: {len(request_data)} chars")
            logger.info(f"AI_DATA_SAMPLE: {request_data[:200]}...")

            # ĞĞ‘ĞĞĞ’Ğ›Ğ•ĞĞĞĞ¯ Ğ¡Ğ¢Ğ ĞĞ¢Ğ•Ğ“Ğ˜Ğ¯: Ollama -> g4f -> GigaChat -> fallback
            ai_providers = [
                ("Ollama", self.answer_with_ollama),
                ("g4f", self.answer_with_g4f),
                ("GigaChat", self.answer_with_gigachat)
            ]

            for provider_name, provider_func in ai_providers:
                logger.info(f"AI_{provider_name.upper()}: Trying {provider_name}...")

                try:
                    if provider_name == "g4f":
                        response = await provider_func(request_data, prompt, timeout=60)
                    else:
                        response = await provider_func(request_data, prompt)

                    if response and self._validate_ai_response(response):
                        logger.info(f"AI_{provider_name.upper()}_SUCCESS: Response received, length: {len(response)}")
                        logger.info(f"AI_{provider_name.upper()}_SAMPLE: {response[:200]}...")
                        return response
                    else:
                        logger.warning(f"AI_{provider_name.upper()}: No valid response from {provider_name}")

                except Exception as e:
                    logger.error(f"AI_{provider_name.upper()}_ERROR: {e}")

            # Final fallback - Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½
            logger.info("AI_FALLBACK: Using fallback template")
            fallback_content = self._create_fallback_test(file_info, framework, test_type)
            logger.info(f"AI_FALLBACK: Generated fallback, length: {len(fallback_content)}")
            return fallback_content

        except Exception as e:
            logger.error(f"AI_ERROR: Test generation failed: {e}", exc_info=True)
            fallback_content = self._create_fallback_test(file_info, framework, test_type)
            logger.info(f"AI_ERROR_FALLBACK: Using fallback due to error")
            return fallback_content

    def _validate_ai_response(self, response: str) -> bool:
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¾Ñ‚ AI"""
        if not response or len(response.strip()) < 50:
            return False
        if any(phrase in response for phrase in ["Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ", "Sorry", "I cannot", "I can't", "ĞºĞ°Ğº AI"]):
            return False
        return True

    def _create_fallback_test(self, file_info: Dict, framework: str, test_type: str) -> str:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ AI"""
        return f"""
# Auto-generated {test_type} test for {file_info.get('path', 'unknown')}
# Generated as fallback (AI service unavailable)

import pytest

def test_basic_functionality():
    \"\"\"Basic test - replace with actual test logic\"\"\"
    assert True
"""

    def _create_test_generation_prompt(self, test_type: str, framework: str, config: Dict) -> str:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°"""

        base_prompt = f"""
        Ğ¢Ñ‹ - ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. 

        ## ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜ Ğ’ĞĞ–ĞĞ«Ğ• ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ:
        1. ğŸ”¥ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¢ĞĞ›Ğ¬ĞšĞ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
        2. ğŸ”¥ ĞĞµ Ğ²Ñ‹Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ¹ Ğ½ĞµÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸, ĞºĞ»Ğ°ÑÑÑ‹ Ğ¸Ğ»Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸  
        3. ğŸ”¥ Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ ĞµÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğµ
        4. ğŸ”¥ Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°

        ## Ğ¢Ğ•Ğ¥ĞĞ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ• Ğ¢Ğ Ğ•Ğ‘ĞĞ’ĞĞĞ˜Ğ¯:
        - Ğ¢Ğ¸Ğ¿ Ñ‚ĞµÑÑ‚Ğ°: {test_type}
        - Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº: {framework}
        - Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸: {config.get('include_comments', True)}

        ## ĞšĞĞ§Ğ•Ğ¡Ğ¢Ğ’Ğ Ğ¢Ğ•Ğ¡Ğ¢ĞĞ’:
        - ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
        - Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¸Ğ¼ĞµĞ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸  
        - ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ assertions/expectations
        - ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° edge cases
        - Ğ¡Ğ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼ Ğ´Ğ»Ñ {framework}
        """

        if test_type == "api":
            base_prompt += f"""
            ## Ğ¡ĞŸĞ•Ğ¦Ğ˜Ğ¤Ğ˜ĞšĞ Ğ”Ğ›Ğ¯ API Ğ¢Ğ•Ğ¡Ğ¢ĞĞ’:
            - Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹ Ğ¸Ğ· Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
            - ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞ¹ ÑÑ‚Ğ°Ñ‚ÑƒÑ ĞºĞ¾Ğ´Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² (200, 201, 400, 401, 404, 500)
            - Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
            - ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ JSON Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²
            - Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞ¹ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğµ)
            - Ğ’ĞºĞ»ÑÑ‡Ğ°Ğ¹ Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ edge cases
            """

        elif test_type == "unit":
            base_prompt += f"""
            ## Ğ¡ĞŸĞ•Ğ¦Ğ˜Ğ¤Ğ˜ĞšĞ Ğ”Ğ›Ğ¯ UNIT Ğ¢Ğ•Ğ¡Ğ¢ĞĞ’:
            - Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞ¹ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ/Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾
            - ĞœĞ¾ĞºĞ¸ Ğ²ÑĞµÑ… Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ (API, DB, File System)  
            - ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞ¹ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ side effects
            - Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ AND Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸
            - Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
            """

        base_prompt += "\n\nĞ’ĞµÑ€Ğ½Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ´ Ñ‚ĞµÑÑ‚Ğ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ±ÑƒĞ´ĞµÑ‚ ÑÑ€Ğ°Ğ·Ñƒ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ² Ñ„Ğ°Ğ¹Ğ» - Ğ»ÑĞ±Ğ¾Ğµ Ğ¸Ğ½Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾ ÑĞ»Ğ¾Ğ¼Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ"
        return base_prompt

    def _get_api_test_example(self, framework: str) -> str:
        """ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ API Ñ‚ĞµÑÑ‚Ğ¾Ğ²"""
        examples = {
            "pytest": '''
    # FastAPI API Test Example
    import pytest
    from fastapi.testclient import TestClient
    from main import app

    client = TestClient(app)

    def test_read_main():
        response = client.get("/")
        assert response.status_code == 200
        assert response.json() == {"message": "Hello World"}

    def test_create_item():
        response = client.post(
            "/items/",
            json={"name": "Test Item", "price": 10.5}
        )
        assert response.status_code == 201
        assert "id" in response.json()

    def test_invalid_input():
        response = client.post(
            "/items/",
            json={"name": ""}  # Invalid empty name
        )
        assert response.status_code == 422
    ''',
            "unittest": '''
    # Flask API Test Example
    import unittest
    from app import create_app

    class TestAPI(unittest.TestCase):
        def setUp(self):
            self.app = create_app('testing')
            self.client = self.app.test_client()

        def test_get_users(self):
            response = self.client.get('/api/users')
            self.assertEqual(response.status_code, 200)
            self.assertIn('users', response.get_json())

        def test_create_user(self):
            user_data = {
                'email': 'test@example.com',
                'password': 'password123'
            }
            response = self.client.post('/api/users', json=user_data)
            self.assertEqual(response.status_code, 201)
    '''
        }
        return examples.get(framework, "")

    def _prepare_test_request_data(self, file_info: Dict, project_context: Dict, test_type: str, framework: str,
                                   config: Dict) -> str:
        """ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ AI Ñ ĞŸĞĞ›ĞĞĞ™ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°"""

        if isinstance(file_info, list):
            if file_info:
                file_info = file_info[0] if isinstance(file_info[0], dict) else {"path": "unknown", "name": "unknown",
                                                                                 "type": "unknown"}
            else:
                file_info = {"path": "unknown", "name": "unknown", "type": "unknown"}

        file_path = file_info.get('path', 'unknown')
        file_name = file_info.get('name', 'unknown')

        project_structure = project_context.get('project_structure', {})
        complete_structure = project_structure.get('complete_file_structure', {})
        file_categories = project_structure.get('file_categories', {})

        request_data = f"""
    ## ğŸ“Š ĞŸĞĞ›ĞĞĞ¯ Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ ĞŸĞ ĞĞ•ĞšĞ¢Ğ:

    ### ğŸ“ˆ Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ:
    - Ğ’ÑĞµĞ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²: {len(complete_structure)}
    - Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´: {len(file_categories.get('source_code', []))}
    - Ğ¢ĞµÑÑ‚Ñ‹: {len(file_categories.get('tests', []))}
    - ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¸: {len(file_categories.get('config_files', []))}
    - Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ: {len(file_categories.get('documentation', []))}

    ### ğŸ—‚ï¸ Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ ĞŸĞ ĞĞ•ĞšĞ¢Ğ:
    """

        dir_structure = {}

        for dir_path, files_data in complete_structure.items():
            if not isinstance(files_data, list):
                continue

            if dir_path not in dir_structure:
                dir_structure[dir_path] = []

            for file_data in files_data:
                if isinstance(file_data, dict):
                    dir_structure[dir_path].append(file_data)

        for directory, files in sorted(dir_structure.items()):
            request_data += f"\nğŸ“ {directory}/:\n"

            try:
                sorted_files = sorted(
                    [f for f in files if isinstance(f, dict)],
                    key=lambda x: x.get('name', '')
                )
            except Exception as e:
                logger.warning(f"Error sorting files in {directory}: {e}")
                sorted_files = files

            for file_data in sorted_files:
                if not isinstance(file_data, dict):
                    continue

                icon = self._get_file_icon(file_data)
                file_name = file_data.get('name', 'unknown')
                request_data += f"   {icon} {file_name}"

                if file_data.get('lines'):
                    request_data += f" ({file_data['lines']} lines)"
                request_data += "\n"

        if file_categories.get('config_files'):
            request_data += f"\n### âš™ï¸ ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜ĞĞĞĞ«Ğ• Ğ¤ĞĞ™Ğ›Ğ«:\n"
            for config_file in file_categories['config_files'][:10]:
                if isinstance(config_file, dict):
                    request_data += f"   - {config_file.get('path', 'unknown')}\n"
                else:
                    request_data += f"   - {config_file}\n"

        request_data += f"""

    ### ğŸ¯ Ğ¢Ğ•ĞšĞ£Ğ©ĞĞ¯ Ğ—ĞĞ”ĞĞ§Ğ:
    Ğ¢Ğ¸Ğ¿ Ñ‚ĞµÑÑ‚Ğ°: {test_type}
    Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº: {framework}
    Ğ¦ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„Ğ°Ğ¹Ğ»: {file_path}

    ### ğŸ“ Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞšĞ¦Ğ˜Ğ˜:
    Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ğ²Ñ‹ÑˆĞµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ².
    Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ· ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ².

    Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞ¹ {test_type.upper()} Ñ‚ĞµÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ {framework.upper()}.

    # Ğ’ĞĞ–ĞĞ!!!! #
    ĞĞµ Ğ¿Ğ¸ÑˆĞ¸ ĞĞ˜Ğ§Ğ•Ğ“Ğ ĞºÑ€Ğ¾Ğ¼Ğµ ĞºĞ¾Ğ´Ğ° - Ğ½Ğ¸ĞºĞ°ĞºĞ¸Ñ… ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŒÑÑĞ½ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² - Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ´. Ñ‚Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ±ÑƒĞ´ĞµÑ‚ ÑÑ€Ğ°Ğ·Ñƒ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ğ² Ñ„Ğ°Ğ¹Ğ»
    Ğ›Ğ®Ğ‘ĞĞ• Ğ»Ğ¸ÑˆĞ½ĞµĞµ ÑĞ»Ğ¾Ğ²Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¡Ğ›ĞĞœĞĞ¢Ğ¬ Ñ„Ğ°Ğ¹Ğ», **ĞŸĞ˜Ğ¨Ğ˜ Ğ¢ĞĞ›Ğ¬ĞšĞ ĞšĞĞ”*
    """
        print(request_data, len(request_data))
        return request_data

    def _get_file_icon(self, file_info: Dict) -> str:
        """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸ĞºĞ¾Ğ½ĞºÑƒ Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°"""
        if file_info.get('is_test'):
            return 'ğŸ§ª'

        file_type = file_info.get('type', '')
        if 'python' in file_type:
            return 'ğŸ'
        elif 'javascript' in file_type or 'react' in file_type:
            return 'ğŸ“œ'
        elif 'java' in file_type:
            return 'â˜•'
        elif 'html' in file_type:
            return 'ğŸŒ'
        elif 'config' in file_type:
            return 'âš™ï¸'
        elif 'documentation' in file_type:
            return 'ğŸ“š'
        else:
            return 'ğŸ“„'


# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ ÑĞµÑ€Ğ²Ğ¸ÑĞ°
ai_service = HybridAIService()