import asyncio
import os
import tempfile
import zipfile
from datetime import datetime
from pathlib import Path
from celery import current_task
from app.celery_app import celery_app
from app.db.session import AsyncSessionLocal
from app.models import Analysis, Project
from app.services.git_service import GitService
from app.services.code_analyzer import CodeAnalyzer
from app.utils.async_utils import robust_async_to_sync
import logging
import shutil

logger = logging.getLogger(__name__)


async def update_analysis_status(analysis_id: int, status: str, error_message: str = None):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤ –ë–î"""
    async with AsyncSessionLocal() as db:
        analysis = await db.get(Analysis, analysis_id)
        if analysis:
            analysis.status = status
            if error_message:
                analysis.error_message = error_message
            await db.commit()
            logger.info(f"‚úÖ Updated analysis {analysis_id} status to {status}")


async def update_analysis_result(analysis_id: int, status: str, result: dict):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤ –ë–î"""
    async with AsyncSessionLocal() as db:
        analysis = await db.get(Analysis, analysis_id)
        if analysis:
            analysis.status = status
            analysis.result = result
            await db.commit()
            logger.info(f"‚úÖ Analysis {analysis_id} result updated in DB")


async def get_project_info(analysis_id: int):
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    async with AsyncSessionLocal() as db:
        analysis = await db.get(Analysis, analysis_id)
        if analysis:
            project = await db.get(Project, analysis.project_id)
            if project:
                return (project.id, project.repo_url, project.branch)
    return None


def _filter_dependencies_from_results(analysis_result: dict) -> dict:
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –í–°–ï –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""

    # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    dependency_indicators = [
        'node_modules', 'vendor', 'bower_components', 'jspm_packages',
        'elm-stuff', 'deps', '__pycache__', '.pytest_cache', '.ruff_cache',
        '.mypy_cache', '.venv', 'venv', 'env', 'virtualenv',
        '.gradle', '.npm', '.yarn', '.pnp', 'Pods', 'DerivedData',
        'build', 'dist', 'out', 'target', 'bin', 'obj',
        '.next', '.nuxt', '.output', '.svelte-kit'
    ]

    # –§–∏–ª—å—Ç—Ä—É–µ–º test_directories
    original_test_dirs = analysis_result['test_analysis']['test_directories']
    filtered_test_dirs = [
        d for d in original_test_dirs
        if not any(dep in d.lower() for dep in dependency_indicators)
    ]

    # –§–∏–ª—å—Ç—Ä—É–µ–º file_structure
    original_files = analysis_result['file_structure']
    filtered_files = {
        k: v for k, v in original_files.items()
        if not any(dep in k.lower() for dep in dependency_indicators)
    }

    # –§–∏–ª—å—Ç—Ä—É–µ–º largest_file –µ—Å–ª–∏ –æ–Ω –∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    largest_file = analysis_result['complexity_metrics']['largest_file']
    if any(dep in largest_file['path'].lower() for dep in dependency_indicators):
        # –ù–∞—Ö–æ–¥–∏–º —Å–ª–µ–¥—É—é—â–∏–π –ø–æ —Ä–∞–∑–º–µ—Ä—É —Ñ–∞–π–ª –Ω–µ –∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        valid_files = [
            (info['path'], info['size'])
            for info in filtered_files.values()
            if info['size'] > 0
        ]
        if valid_files:
            valid_files.sort(key=lambda x: x[1], reverse=True)
            analysis_result['complexity_metrics']['largest_file'] = {
                'path': valid_files[0][0],
                'size': valid_files[0][1]
            }
        else:
            analysis_result['complexity_metrics']['largest_file'] = {'path': '', 'size': 0}

    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    analysis_result['test_analysis']['test_directories'] = filtered_test_dirs
    analysis_result['file_structure'] = filtered_files

    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
    analysis_result['metrics']['total_files'] = len(filtered_files)
    analysis_result['metrics']['code_files'] = sum(
        1 for file_info in filtered_files.values()
        if file_info.get('technology') and not file_info.get('is_test')
    )
    analysis_result['metrics']['test_files'] = sum(
        1 for file_info in filtered_files.values()
        if file_info.get('is_test')
    )

    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∏ —Ä–∞–∑–º–µ—Ä
    total_lines = 0
    total_size = 0
    for file_info in filtered_files.values():
        total_lines += file_info.get('lines', 0)
        total_size += file_info.get('size', 0)

    analysis_result['metrics']['total_lines'] = total_lines
    analysis_result['metrics']['total_size_kb'] = total_size / 1024

    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    if analysis_result['metrics']['code_files'] > 0:
        analysis_result['complexity_metrics']['avg_file_size'] = total_size / analysis_result['metrics']['code_files']
    else:
        analysis_result['complexity_metrics']['avg_file_size'] = 0

    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    extension_counts = {}
    for file_info in filtered_files.values():
        ext = file_info.get('extension', '')
        if ext:
            extension_counts[ext] = extension_counts.get(ext, 0) + 1
    analysis_result['complexity_metrics']['file_extensions'] = extension_counts

    logger.info(f"üîç After dependency filtering:")
    logger.info(f"   - Project files: {len(filtered_files)}")
    logger.info(f"   - Removed {len(original_test_dirs) - len(filtered_test_dirs)} test dirs from dependencies")
    logger.info(f"   - Code files: {analysis_result['metrics']['code_files']}")
    logger.info(f"   - Test files: {analysis_result['metrics']['test_files']}")

    return analysis_result


async def perform_repository_analysis(analysis_id: int):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è"""
    try:
        logger.info(f"üîç Starting REAL repository analysis for ID: {analysis_id}")

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ –ë–î
        await update_analysis_status(analysis_id, "cloning")

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ
        project_info = await get_project_info(analysis_id)
        if not project_info:
            raise Exception("Project not found")

        project_id, repo_url, branch = project_info

        logger.info(f"üì¶ Cloning repository: {repo_url}, branch: {branch}")

        # –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
        git_service = GitService()
        repo_path = await git_service.clone_repository(repo_url, branch)

        logger.info(f"‚úÖ Repository cloned to: {repo_path}")

        try:
            await update_analysis_status(analysis_id, "analyzing")

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–¥ —Å —Ä–µ–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º
            code_analyzer = CodeAnalyzer()
            analysis_result = await code_analyzer.analyze_repository(repo_path)

            # üî• –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –í–°–ï–• –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô
            analysis_result = _filter_dependencies_from_results(analysis_result)

            logger.info(f"üìä REAL analysis completed (after dependency filtering):")
            logger.info(f"  - Technologies: {analysis_result.get('technologies', [])}")
            logger.info(f"  - Frameworks: {analysis_result.get('frameworks', [])}")
            logger.info(f"  - Project files: {analysis_result['metrics']['total_files']}")
            logger.info(f"  - Code files: {analysis_result['metrics']['code_files']}")
            logger.info(f"  - Test files: {analysis_result['metrics']['test_files']}")
            logger.info(f"  - Ignored dependency files: {analysis_result['metrics']['ignored_files']}")

            await update_analysis_status(analysis_id, "generating")

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º coverage –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            coverage_estimate = _calculate_real_coverage(analysis_result)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¢–û–õ–¨–ö–û –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞
            result_data = {
                "technologies": analysis_result.get('technologies', []),
                "frameworks": analysis_result.get('frameworks', []),
                "file_structure_summary": {
                    "total_files": analysis_result['metrics']['total_files'],
                    "code_files": analysis_result['metrics']['code_files'],
                    "test_files": analysis_result['metrics']['test_files'],
                    "total_lines": analysis_result['metrics']['total_lines'],
                    "total_size_kb": round(analysis_result['metrics']['total_size_kb'], 2),
                },
                "test_analysis": {
                    "has_tests": analysis_result['test_analysis']['has_tests'],
                    "test_frameworks": analysis_result['test_analysis']['test_frameworks'],
                    "test_files_count": analysis_result['test_analysis']['test_files_count'],
                    "test_directories": analysis_result['test_analysis']['test_directories'],
                },
                "project_structure": analysis_result['project_structure'],
                "dependencies": analysis_result['dependencies'],
                "complexity_metrics": analysis_result['complexity_metrics'],
                "source": repo_url,
                "branch": branch,
                "coverage_estimate": coverage_estimate,
                "analysis_timestamp": datetime.utcnow().isoformat()
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –†–ï–ê–õ–¨–ù–´–ô —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ë–î
            await update_analysis_result(
                analysis_id,
                "completed",
                result_data
            )

            logger.info(f"‚úÖ REAL Repository analysis {analysis_id} completed successfully")
            logger.info(f"üìà Final coverage estimate: {coverage_estimate}%")

            return {
                "status": "success",
                "analysis_id": analysis_id,
                "technologies": analysis_result.get('technologies', []),
                "has_tests": analysis_result['test_analysis']['has_tests'],
                "test_frameworks": analysis_result['test_analysis']['test_frameworks'],
                "coverage": coverage_estimate
            }

        except Exception as e:
            logger.error(f"‚ùå Analysis failed: {e}")
            await update_analysis_status(analysis_id, "failed", str(e))
            raise
        finally:
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            git_service.cleanup(repo_path)

    except Exception as e:
        logger.error(f"‚ùå Repository analysis {analysis_id} failed: {str(e)}")
        await update_analysis_status(analysis_id, "failed", str(e))
        raise


async def perform_zip_analysis(analysis_id: int, zip_path: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ ZIP –∞—Ä—Ö–∏–≤–∞"""
    try:
        logger.info(f"üì¶ Starting REAL ZIP analysis for ID: {analysis_id}")

        await update_analysis_status(analysis_id, "extracting")

        # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º ZIP
        extract_path = tempfile.mkdtemp(prefix="extracted_")

        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)

        logger.info(f"‚úÖ ZIP extracted to: {extract_path}")

        try:
            await update_analysis_status(analysis_id, "analyzing")

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–¥ —Å —Ä–µ–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º
            code_analyzer = CodeAnalyzer()
            analysis_result = await code_analyzer.analyze_repository(extract_path)

            # üî• –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –í–°–ï–• –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô
            analysis_result = _filter_dependencies_from_results(analysis_result)

            logger.info(f"üìä REAL ZIP analysis completed:")
            logger.info(f"  - Technologies: {analysis_result.get('technologies', [])}")
            logger.info(f"  - Project files: {analysis_result['metrics']['total_files']}")
            logger.info(f"  - Test files: {analysis_result['metrics']['test_files']}")

            await update_analysis_status(analysis_id, "generating")

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º coverage –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            coverage_estimate = _calculate_real_coverage(analysis_result)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            result_data = {
                "technologies": analysis_result.get('technologies', []),
                "frameworks": analysis_result.get('frameworks', []),
                "file_structure_summary": {
                    "total_files": analysis_result['metrics']['total_files'],
                    "code_files": analysis_result['metrics']['code_files'],
                    "test_files": analysis_result['metrics']['test_files'],
                    "total_lines": analysis_result['metrics']['total_lines'],
                    "total_size_kb": round(analysis_result['metrics']['total_size_kb'], 2),
                },
                "test_analysis": {
                    "has_tests": analysis_result['test_analysis']['has_tests'],
                    "test_frameworks": analysis_result['test_analysis']['test_frameworks'],
                    "test_files_count": analysis_result['test_analysis']['test_files_count'],
                    "test_directories": analysis_result['test_analysis']['test_directories'],
                },
                "project_structure": analysis_result['project_structure'],
                "dependencies": analysis_result['dependencies'],
                "complexity_metrics": analysis_result['complexity_metrics'],
                "source": "ZIP Archive",
                "branch": "main",
                "coverage_estimate": coverage_estimate,
                "analysis_timestamp": datetime.utcnow().isoformat()
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            await update_analysis_result(
                analysis_id,
                "completed",
                result_data
            )

            logger.info(f"‚úÖ ZIP analysis {analysis_id} completed")

            return {
                "status": "success",
                "analysis_id": analysis_id,
                "technologies": analysis_result.get('technologies', []),
                "has_tests": analysis_result['test_analysis']['has_tests'],
                "test_frameworks": analysis_result['test_analysis']['test_frameworks'],
                "coverage": coverage_estimate
            }

        finally:
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            shutil.rmtree(extract_path, ignore_errors=True)
            if os.path.exists(zip_path):
                os.remove(zip_path)

    except Exception as e:
        logger.error(f"‚ùå ZIP analysis {analysis_id} failed: {str(e)}")
        await update_analysis_status(analysis_id, "failed", str(e))
        raise


@celery_app.task(bind=True, name="app.tasks.analyze_repository_task")
@robust_async_to_sync
async def analyze_repository_task(self, analysis_id: int):
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –∏–∑ GitHub"""
    logger.info(f"üéØ Starting analyze_repository_task for analysis_id: {analysis_id}")
    try:
        result = await perform_repository_analysis(analysis_id)
        logger.info(f"‚úÖ analyze_repository_task completed for analysis_id: {analysis_id}")
        return result
    except Exception as e:
        logger.error(f"‚ùå analyze_repository_task failed for analysis_id {analysis_id}: {str(e)}")
        return {
            "status": "error",
            "analysis_id": analysis_id,
            "error": str(e)
        }


@celery_app.task(bind=True, name="app.tasks.analyze_zip_task")
@robust_async_to_sync
async def analyze_zip_task(self, analysis_id: int, zip_path: str):
    """–ê–Ω–∞–ª–∏–∑ ZIP –∞—Ä—Ö–∏–≤–∞"""
    logger.info(f"üéØ Starting analyze_zip_task for analysis_id: {analysis_id}")
    try:
        result = await perform_zip_analysis(analysis_id, zip_path)
        logger.info(f"‚úÖ analyze_zip_task completed for analysis_id: {analysis_id}")
        return result
    except Exception as e:
        logger.error(f"‚ùå analyze_zip_task failed for analysis_id {analysis_id}: {str(e)}")
        return {
            "status": "error",
            "analysis_id": analysis_id,
            "error": str(e)
        }


@celery_app.task(bind=True, name="app.tasks.test_dependency_filtering_task")
@robust_async_to_sync
async def test_dependency_filtering_task(self, repo_url: str, branch: str = "main"):
    """–¢–µ—Å—Ç–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –í–°–ï–• –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    logger.info(f"üß™ Testing dependency filtering with: {repo_url}, branch: {branch}")

    try:
        git_service = GitService()
        repo_path = await git_service.clone_repository(repo_url, branch)

        code_analyzer = CodeAnalyzer()
        analysis_result = await code_analyzer.analyze_repository(repo_path)

        # –õ–æ–≥–∏—Ä—É–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –î–û —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        logger.info(f"üß™ BEFORE DEPENDENCY FILTERING:")
        logger.info(f"üß™ Total files found: {analysis_result['metrics']['total_files']}")
        logger.info(f"üß™ Ignored dependency files: {analysis_result['metrics']['ignored_files']}")
        logger.info(f"üß™ Dependency files count: {analysis_result['metrics']['dependency_files_count']}")

        test_dirs_before = analysis_result['test_analysis']['test_directories']
        dependency_dirs_before = [
            d for d in test_dirs_before
            if any(dep in d.lower() for dep in [
                'node_modules', 'vendor', 'bower_components', '__pycache__',
                '.venv', 'venv', '.gradle', '.yarn'
            ])
        ]
        logger.info(f"üß™ Dependency test dirs before: {len(dependency_dirs_before)}")

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –í–°–ï–• –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        analysis_result = _filter_dependencies_from_results(analysis_result)

        # –õ–æ–≥–∏—Ä—É–µ–º –ü–û–°–õ–ï —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        logger.info(f"üß™ AFTER DEPENDENCY FILTERING:")
        logger.info(f"üß™ Clean project files: {analysis_result['metrics']['total_files']}")
        logger.info(f"üß™ Clean test files: {analysis_result['metrics']['test_files']}")

        test_dirs_after = analysis_result['test_analysis']['test_directories']
        dependency_dirs_after = [
            d for d in test_dirs_after
            if any(dep in d.lower() for dep in [
                'node_modules', 'vendor', 'bower_components', '__pycache__',
                '.venv', 'venv', '.gradle', '.yarn'
            ])
        ]
        logger.info(f"üß™ Dependency test dirs after: {len(dependency_dirs_after)}")

        if dependency_dirs_after:
            logger.error(f"‚ùå DEPENDENCIES STILL FOUND: {dependency_dirs_after}")
            return {
                "status": "error",
                "message": "Dependencies still present in results",
                "remaining_dependencies": dependency_dirs_after
            }
        else:
            logger.info("‚úÖ SUCCESS: All dependencies filtered out!")
            return {
                "status": "success",
                "message": "All dependencies successfully filtered",
                "before_filtering": {
                    "total_files": analysis_result['metrics']['total_files'],
                    "ignored_files": analysis_result['metrics']['ignored_files'],
                    "dependency_test_dirs": len(dependency_dirs_before)
                },
                "after_filtering": {
                    "project_files": analysis_result['metrics']['total_files'],
                    "test_files": analysis_result['metrics']['test_files'],
                    "dependency_test_dirs": len(dependency_dirs_after)
                }
            }

    except Exception as e:
        logger.error(f"‚ùå Dependency filtering test failed: {e}")
        raise
    finally:
        if 'repo_path' in locals():
            git_service.cleanup(repo_path)


@celery_app.task(bind=True, name="app.tasks.diagnostic_task")
@robust_async_to_sync
async def diagnostic_task(self, test_type: str = "basic"):
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏"""
    logger.info(f"üîß Starting diagnostic task: {test_type}")

    try:
        if test_type == "basic":
            logger.info("üîÑ Testing asyncio.sleep...")
            await asyncio.sleep(2)
            logger.info("‚úÖ asyncio.sleep completed")
            return {"status": "success", "test": "basic_async"}

        elif test_type == "db":
            logger.info("üîÑ Testing database connection...")
            async with AsyncSessionLocal() as db:
                result = await db.execute("SELECT 1")
                value = result.scalar()
                logger.info(f"‚úÖ Database test completed: {value}")
            return {"status": "success", "test": "database"}

        elif test_type == "git":
            logger.info("üîÑ Testing Git operations...")
            git_service = GitService()
            repo_path = await git_service.clone_repository(
                "https://github.com/octocat/Hello-World",
                "main"
            )
            logger.info(f"‚úÖ Git clone completed: {repo_path}")
            git_service.cleanup(repo_path)
            return {"status": "success", "test": "git"}

    except Exception as e:
        logger.error(f"‚ùå Diagnostic task failed: {e}", exc_info=True)
        return {"status": "error", "test": test_type, "error": str(e)}


def _calculate_real_coverage(analysis_result):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç coverage –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ —Ç–µ—Å—Ç–∞—Ö"""
    if not analysis_result['test_analysis']['has_tests']:
        return 0

    total_files = analysis_result['metrics']['code_files']
    test_files = analysis_result['metrics']['test_files']

    if total_files == 0:
        return 0

    # –ë–∞–∑–æ–≤—ã–π —Ä–∞—Å—á–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∫ –æ–±—â–∏–º
    file_coverage_ratio = test_files / total_files

    # –£—á–∏—Ç—ã–≤–∞–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤
    framework_bonus = len(analysis_result['test_analysis']['test_frameworks']) * 5

    # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–µ—Å—Ç–æ–≤)
    structure_bonus = len(analysis_result['test_analysis']['test_directories']) * 3

    coverage = min(85, int(file_coverage_ratio * 70 + framework_bonus + structure_bonus))

    return max(10, coverage)  # –ú–∏–Ω–∏–º—É–º 10% –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ—Å—Ç—ã