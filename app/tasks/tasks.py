import asyncio
import os
import tempfile
import zipfile
from datetime import datetime
import time
from typing import List
from sqlalchemy import select
# from pathlib import Path
# from celery import current_task
from celery import group, chord
from app.celery_app import celery_app
from app.db.session import AsyncSessionLocal
from app.models import Analysis, Project
from app.services.git_service import GitService
from app.services.code_analyzer import CodeAnalyzer
from app.utils.async_utils import robust_async_to_sync
import logging
import shutil

logger = logging.getLogger(__name__)


async def update_analysis_status(analysis_id: int, status: str, error_message: str = None):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤ –ë–î"""
    async with AsyncSessionLocal() as db:
        analysis = await db.get(Analysis, analysis_id)
        if analysis:
            analysis.status = status
            if error_message:
                analysis.error_message = error_message
            await db.commit()
            logger.info(f"‚úÖ Updated analysis {analysis_id} status to {status}")


async def update_analysis_result(analysis_id: int, status: str, result: dict):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤ –ë–î"""
    async with AsyncSessionLocal() as db:
        analysis = await db.get(Analysis, analysis_id)
        if analysis:
            analysis.status = status
            analysis.result = result
            await db.commit()
            logger.info(f"‚úÖ Analysis {analysis_id} result updated in DB")


async def get_project_info(analysis_id: int):
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    async with AsyncSessionLocal() as db:
        analysis = await db.get(Analysis, analysis_id)
        if analysis:
            project = await db.get(Project, analysis.project_id)
            if project:
                return (project.id, project.repo_url, project.branch)
    return None


def _filter_dependencies_from_results(analysis_result: dict) -> dict:
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –í–°–ï –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""

    # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    dependency_indicators = [
        'node_modules', 'vendor', 'bower_components', 'jspm_packages',
        'elm-stuff', 'deps', '__pycache__', '.pytest_cache', '.ruff_cache',
        '.mypy_cache', '.venv', 'venv', 'env', 'virtualenv',
        '.gradle', '.npm', '.yarn', '.pnp', 'Pods', 'DerivedData',
        'build', 'dist', 'out', 'target', 'bin', 'obj',
        '.next', '.nuxt', '.output', '.svelte-kit'
    ]

    # –§–∏–ª—å—Ç—Ä—É–µ–º test_directories
    original_test_dirs = analysis_result['test_analysis']['test_directories']
    filtered_test_dirs = [
        d for d in original_test_dirs
        if not any(dep in d.lower() for dep in dependency_indicators)
    ]

    # –§–∏–ª—å—Ç—Ä—É–µ–º file_structure
    original_files = analysis_result['file_structure']
    filtered_files = {
        k: v for k, v in original_files.items()
        if not any(dep in k.lower() for dep in dependency_indicators)
    }

    # –§–∏–ª—å—Ç—Ä—É–µ–º largest_file –µ—Å–ª–∏ –æ–Ω –∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    largest_file = analysis_result['complexity_metrics']['largest_file']
    if any(dep in largest_file['path'].lower() for dep in dependency_indicators):
        # –ù–∞—Ö–æ–¥–∏–º —Å–ª–µ–¥—É—é—â–∏–π –ø–æ —Ä–∞–∑–º–µ—Ä—É —Ñ–∞–π–ª –Ω–µ –∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        valid_files = [
            (info['path'], info['size'])
            for info in filtered_files.values()
            if info['size'] > 0
        ]
        if valid_files:
            valid_files.sort(key=lambda x: x[1], reverse=True)
            analysis_result['complexity_metrics']['largest_file'] = {
                'path': valid_files[0][0],
                'size': valid_files[0][1]
            }
        else:
            analysis_result['complexity_metrics']['largest_file'] = {'path': '', 'size': 0}

    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    analysis_result['test_analysis']['test_directories'] = filtered_test_dirs
    analysis_result['file_structure'] = filtered_files

    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
    analysis_result['metrics']['total_files'] = len(filtered_files)
    analysis_result['metrics']['code_files'] = sum(
        1 for file_info in filtered_files.values()
        if file_info.get('technology') and not file_info.get('is_test')
    )
    analysis_result['metrics']['test_files'] = sum(
        1 for file_info in filtered_files.values()
        if file_info.get('is_test')
    )

    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∏ —Ä–∞–∑–º–µ—Ä
    total_lines = 0
    total_size = 0
    for file_info in filtered_files.values():
        total_lines += file_info.get('lines', 0)
        total_size += file_info.get('size', 0)

    analysis_result['metrics']['total_lines'] = total_lines
    analysis_result['metrics']['total_size_kb'] = total_size / 1024

    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    if analysis_result['metrics']['code_files'] > 0:
        analysis_result['complexity_metrics']['avg_file_size'] = total_size / analysis_result['metrics']['code_files']
    else:
        analysis_result['complexity_metrics']['avg_file_size'] = 0

    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    extension_counts = {}
    for file_info in filtered_files.values():
        ext = file_info.get('extension', '')
        if ext:
            extension_counts[ext] = extension_counts.get(ext, 0) + 1
    analysis_result['complexity_metrics']['file_extensions'] = extension_counts

    logger.info(f"üîç After dependency filtering:")
    logger.info(f"   - Project files: {len(filtered_files)}")
    logger.info(f"   - Removed {len(original_test_dirs) - len(filtered_test_dirs)} test dirs from dependencies")
    logger.info(f"   - Code files: {analysis_result['metrics']['code_files']}")
    logger.info(f"   - Test files: {analysis_result['metrics']['test_files']}")

    return analysis_result


async def perform_repository_analysis(analysis_id: int):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è"""
    repo_path = None
    try:
        logger.info(f"üîç Starting REAL repository analysis for ID: {analysis_id}")

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ –ë–î
        await update_analysis_status(analysis_id, "cloning")

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ
        project_info = await get_project_info(analysis_id)
        if not project_info:
            raise Exception("Project not found")

        project_id, repo_url, branch = project_info

        logger.info(f"üì¶ Cloning repository: {repo_url}, branch: {branch}")

        # –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (–≤—Å–µ–≥–¥–∞ –Ω–æ–≤–∞—è –∫–æ–ø–∏—è)
        git_service = GitService()
        repo_path = await git_service.clone_repository(repo_url, branch)

        logger.info(f"‚úÖ Repository cloned to: {repo_path}")

        try:
            await update_analysis_status(analysis_id, "analyzing")

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–¥ —Å —Ä–µ–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º
            code_analyzer = CodeAnalyzer()
            analysis_result = await code_analyzer.analyze_repository(repo_path)

            # üî• –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –í–°–ï–• –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô
            analysis_result = _filter_dependencies_from_results(analysis_result)

            logger.info(f"üìä REAL analysis completed:")
            logger.info(f"  - Technologies: {analysis_result.get('technologies', [])}")
            logger.info(f"  - Frameworks: {analysis_result.get('frameworks', [])}")
            logger.info(f"  - Project files: {analysis_result['metrics']['total_files']}")
            logger.info(f"  - Code files: {analysis_result['metrics']['code_files']}")
            logger.info(f"  - Test files: {analysis_result['metrics']['test_files']}")

            await update_analysis_status(analysis_id, "generating")

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º coverage –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            coverage_estimate = _calculate_real_coverage(analysis_result)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¢–û–õ–¨–ö–û –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞
            result_data = {
                "technologies": analysis_result.get('technologies', []),
                "frameworks": analysis_result.get('frameworks', []),
                "file_structure_summary": {
                    "total_files": analysis_result['metrics']['total_files'],
                    "code_files": analysis_result['metrics']['code_files'],
                    "test_files": analysis_result['metrics']['test_files'],
                    "total_lines": analysis_result['metrics']['total_lines'],
                    "total_size_kb": round(analysis_result['metrics']['total_size_kb'], 2),
                },
                "test_analysis": {
                    "has_tests": analysis_result['test_analysis']['has_tests'],
                    "test_frameworks": analysis_result['test_analysis']['test_frameworks'],
                    "test_files_count": analysis_result['test_analysis']['test_files_count'],
                    "test_directories": analysis_result['test_analysis']['test_directories'],
                },
                "project_structure": analysis_result['project_structure'],
                "dependencies": analysis_result['dependencies'],
                "complexity_metrics": analysis_result['complexity_metrics'],
                "source": repo_url,
                "branch": branch,
                "coverage_estimate": coverage_estimate,
                "analysis_timestamp": datetime.utcnow().isoformat()
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –†–ï–ê–õ–¨–ù–´–ô —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ë–î
            await update_analysis_result(
                analysis_id,
                "completed",
                result_data
            )

            logger.info(f"‚úÖ REAL Repository analysis {analysis_id} completed successfully")
            logger.info(f"üìà Final coverage estimate: {coverage_estimate}%")

            return {
                "status": "success",
                "analysis_id": analysis_id,
                "technologies": analysis_result.get('technologies', []),
                "has_tests": analysis_result['test_analysis']['has_tests'],
                "test_frameworks": analysis_result['test_analysis']['test_frameworks'],
                "coverage": coverage_estimate
            }

        except Exception as e:
            logger.error(f"‚ùå Analysis failed: {e}")
            await update_analysis_status(analysis_id, "failed", str(e))
            raise

    except Exception as e:
        logger.error(f"‚ùå Repository analysis {analysis_id} failed: {str(e)}")
        await update_analysis_status(analysis_id, "failed", str(e))
        raise
    finally:
        if repo_path and os.path.exists(repo_path):
            logger.info(f"üßπ Cleaning up temporary repository: {repo_path}")
            git_service = GitService()
            git_service.cleanup(repo_path)
        else:
            logger.info(f"‚ö†Ô∏è  No temporary repository to clean up for analysis {analysis_id}")


async def perform_zip_analysis(analysis_id: int, zip_path: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ ZIP –∞—Ä—Ö–∏–≤–∞"""
    try:
        logger.info(f"üì¶ Starting REAL ZIP analysis for ID: {analysis_id}")

        await update_analysis_status(analysis_id, "extracting")

        # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º ZIP
        extract_path = tempfile.mkdtemp(prefix="extracted_")

        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)

        logger.info(f"‚úÖ ZIP extracted to: {extract_path}")

        try:
            await update_analysis_status(analysis_id, "analyzing")

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–¥ —Å —Ä–µ–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º
            code_analyzer = CodeAnalyzer()
            analysis_result = await code_analyzer.analyze_repository(extract_path)

            # üî• –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –í–°–ï–• –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô
            analysis_result = _filter_dependencies_from_results(analysis_result)

            logger.info(f"üìä REAL ZIP analysis completed:")
            logger.info(f"  - Technologies: {analysis_result.get('technologies', [])}")
            logger.info(f"  - Project files: {analysis_result['metrics']['total_files']}")
            logger.info(f"  - Test files: {analysis_result['metrics']['test_files']}")

            await update_analysis_status(analysis_id, "generating")

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º coverage –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            coverage_estimate = _calculate_real_coverage(analysis_result)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            result_data = {
                "technologies": analysis_result.get('technologies', []),
                "frameworks": analysis_result.get('frameworks', []),
                "file_structure_summary": {
                    "total_files": analysis_result['metrics']['total_files'],
                    "code_files": analysis_result['metrics']['code_files'],
                    "test_files": analysis_result['metrics']['test_files'],
                    "total_lines": analysis_result['metrics']['total_lines'],
                    "total_size_kb": round(analysis_result['metrics']['total_size_kb'], 2),
                },
                "test_analysis": {
                    "has_tests": analysis_result['test_analysis']['has_tests'],
                    "test_frameworks": analysis_result['test_analysis']['test_frameworks'],
                    "test_files_count": analysis_result['test_analysis']['test_files_count'],
                    "test_directories": analysis_result['test_analysis']['test_directories'],
                },
                "project_structure": analysis_result['project_structure'],
                "dependencies": analysis_result['dependencies'],
                "complexity_metrics": analysis_result['complexity_metrics'],
                "source": "ZIP Archive",
                "branch": "main",
                "coverage_estimate": coverage_estimate,
                "analysis_timestamp": datetime.utcnow().isoformat()
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            await update_analysis_result(
                analysis_id,
                "completed",
                result_data
            )

            logger.info(f"‚úÖ ZIP analysis {analysis_id} completed")

            return {
                "status": "success",
                "analysis_id": analysis_id,
                "technologies": analysis_result.get('technologies', []),
                "has_tests": analysis_result['test_analysis']['has_tests'],
                "test_frameworks": analysis_result['test_analysis']['test_frameworks'],
                "coverage": coverage_estimate
            }

        finally:
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            shutil.rmtree(extract_path, ignore_errors=True)
            if os.path.exists(zip_path):
                os.remove(zip_path)

    except Exception as e:
        logger.error(f"‚ùå ZIP analysis {analysis_id} failed: {str(e)}")
        await update_analysis_status(analysis_id, "failed", str(e))
        raise


# =============================================================================
# –û–°–ù–û–í–ù–´–ï –ó–ê–î–ê–ß–ò –ê–ù–ê–õ–ò–ó–ê
# =============================================================================

@celery_app.task(bind=True, name="app.tasks.analyze_repository_task")
@robust_async_to_sync
async def analyze_repository_task(self, analysis_id: int):
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –∏–∑ GitHub"""
    start_time = time.time()
    logger.info(f"üéØ Starting analyze_repository_task for analysis_id: {analysis_id}")

    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        self.update_state(
            state='PROGRESS',
            meta={'current': 0, 'total': 100, 'status': 'starting'}
        )

        result = await perform_repository_analysis(analysis_id)
        execution_time = time.time() - start_time

        logger.info(f"‚úÖ Analysis {analysis_id} completed in {execution_time:.2f}s")
        return result

    except Exception as e:
        execution_time = time.time() - start_time
        logger.error(f"‚ùå Analysis {analysis_id} failed after {execution_time:.2f}s: {e}")

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ –ë–î –ø—Ä–∏ –æ—à–∏–±–∫–µ
        try:
            await update_analysis_status(analysis_id, "failed", str(e))
        except Exception as db_error:
            logger.error(f"Failed to update analysis status: {db_error}")

        return {
            "status": "error",
            "analysis_id": analysis_id,
            "error": str(e),
            "execution_time": execution_time
        }


@celery_app.task(bind=True, name="app.tasks.analyze_zip_task")
@robust_async_to_sync
async def analyze_zip_task(self, analysis_id: int, zip_path: str):
    """–ê–Ω–∞–ª–∏–∑ ZIP –∞—Ä—Ö–∏–≤–∞"""
    start_time = time.time()
    logger.info(f"üéØ Starting analyze_zip_task for analysis_id: {analysis_id}")

    try:
        self.update_state(
            state='PROGRESS',
            meta={'current': 0, 'total': 100, 'status': 'starting'}
        )

        result = await perform_zip_analysis(analysis_id, zip_path)
        execution_time = time.time() - start_time

        logger.info(f"‚úÖ ZIP analysis {analysis_id} completed in {execution_time:.2f}s")
        return result

    except Exception as e:
        execution_time = time.time() - start_time
        logger.error(f"‚ùå ZIP analysis {analysis_id} failed after {execution_time:.2f}s: {str(e)}")

        try:
            await update_analysis_status(analysis_id, "failed", str(e))
        except Exception as db_error:
            logger.error(f"Failed to update analysis status: {db_error}")

        return {
            "status": "error",
            "analysis_id": analysis_id,
            "error": str(e),
            "execution_time": execution_time
        }


# =============================================================================
# –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ï –ó–ê–î–ê–ß–ò –î–õ–Ø –ì–†–£–ü–ü–û–í–û–ô –û–ë–†–ê–ë–û–¢–ö–ò
# =============================================================================

@celery_app.task(bind=True, name="app.tasks.batch_analyze_repositories_task")
def batch_analyze_repositories_task(self, analysis_ids: List[int]):
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤"""
    logger.info(f"üöÄ Starting batch analysis for {len(analysis_ids)} repositories")

    # –°–æ–∑–¥–∞–µ–º –≥—Ä—É–ø–ø—É –∑–∞–¥–∞—á –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    job = group(
        analyze_repository_task.s(analysis_id)
        for analysis_id in analysis_ids
    )

    result = job.apply_async()

    return {
        "status": "started",
        "total_tasks": len(analysis_ids),
        "task_group_id": result.id,
        "analysis_ids": analysis_ids
    }


@celery_app.task(bind=True, name="app.tasks.batch_analyze_zips_task")
def batch_analyze_zips_task(self, analysis_data: List[dict]):
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö ZIP –∞—Ä—Ö–∏–≤–æ–≤"""
    logger.info(f"üöÄ Starting batch ZIP analysis for {len(analysis_data)} archives")

    tasks = []
    for data in analysis_data:
        task = analyze_zip_task.s(data['analysis_id'], data['zip_path'])
        tasks.append(task)

    job = group(tasks)
    result = job.apply_async()

    return {
        "status": "started",
        "total_tasks": len(analysis_data),
        "task_group_id": result.id,
        "analysis_data": analysis_data
    }


# =============================================================================
# –ó–ê–î–ê–ß–ò –ì–ï–ù–ï–†–ê–¶–ò–ò –¢–ï–°–¢–û–í
# =============================================================================

@celery_app.task(bind=True, name="app.tasks.parallel_test_generation_task")
@robust_async_to_sync
async def parallel_test_generation_task(self, project_id: int, test_config: dict):
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ç–µ—Å—Ç–æ–≤"""
    try:
        from app.services.generate_pipeline import test_generation_pipeline

        logger.info(f"üöÄ Starting parallel test generation for project {project_id}")

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–µ–∫—Ç –∏ –∞–Ω–∞–ª–∏–∑
        async with AsyncSessionLocal() as db:
            project = await db.get(Project, project_id)
            if not project:
                raise Exception("Project not found")

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞–Ω–∞–ª–∏–∑
            analysis_result = await db.execute(
                select(Analysis)
                .where(
                    Analysis.project_id == project_id,
                    Analysis.status == "completed"
                )
                .order_by(Analysis.created_at.desc())
                .limit(1)
            )
            analysis = analysis_result.scalar_one_or_none()

        if not analysis:
            raise Exception("No completed analysis found")

        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ç–µ—Å—Ç–æ–≤
        tasks = []

        if test_config.get("generate_unit_tests", True):
            unit_task = generate_unit_tests_task.s(project_id, test_config, analysis.result)
            tasks.append(unit_task)

        if test_config.get("generate_integration_tests", True):
            integration_task = generate_integration_tests_task.s(project_id, test_config, analysis.result)
            tasks.append(integration_task)

        if test_config.get("generate_e2e_tests", False):
            e2e_task = generate_e2e_tests_task.s(project_id, test_config, analysis.result)
            tasks.append(e2e_task)

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        if tasks:
            job = group(tasks)
            group_result = job.apply_async()

            return {
                "status": "parallel_generation_started",
                "project_id": project_id,
                "task_group_id": group_result.id,
                "task_count": len(tasks)
            }
        else:
            return {"status": "no_tasks_created", "project_id": project_id}

    except Exception as e:
        logger.error(f"‚ùå Parallel test generation failed: {e}")
        raise


@celery_app.task(bind=True, name="app.tasks.generate_unit_tests_task")
@robust_async_to_sync
async def generate_unit_tests_task(self, project_id: int, test_config: dict, analysis_data: dict):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è unit —Ç–µ—Å—Ç–æ–≤ - –æ—Ç–¥–µ–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞"""
    start_time = time.time()
    logger.info(f"üîß Generating unit tests for project {project_id}")

    try:
        # –ò–º–∏—Ç–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ unit —Ç–µ—Å—Ç–æ–≤ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É)
        self.update_state(state='PROGRESS', meta={'status': 'generating_unit_tests'})
        await asyncio.sleep(5)

        execution_time = time.time() - start_time

        return {
            "status": "success",
            "test_type": "unit",
            "project_id": project_id,
            "generated_tests": 15,
            "coverage_estimate": 65,
            "execution_time": execution_time
        }
    except Exception as e:
        logger.error(f"‚ùå Unit test generation failed: {e}")
        raise


@celery_app.task(bind=True, name="app.tasks.generate_integration_tests_task")
@robust_async_to_sync
async def generate_integration_tests_task(self, project_id: int, test_config: dict, analysis_data: dict):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ - –æ—Ç–¥–µ–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞"""
    start_time = time.time()
    logger.info(f"üîß Generating integration tests for project {project_id}")

    try:
        # –ò–º–∏—Ç–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É)
        self.update_state(state='PROGRESS', meta={'status': 'generating_integration_tests'})
        await asyncio.sleep(3)

        execution_time = time.time() - start_time

        return {
            "status": "success",
            "test_type": "integration",
            "project_id": project_id,
            "generated_tests": 8,
            "coverage_estimate": 25,
            "execution_time": execution_time
        }
    except Exception as e:
        logger.error(f"‚ùå Integration test generation failed: {e}")
        raise


@celery_app.task(bind=True, name="app.tasks.generate_e2e_tests_task")
@robust_async_to_sync
async def generate_e2e_tests_task(self, project_id: int, test_config: dict, analysis_data: dict):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è E2E —Ç–µ—Å—Ç–æ–≤ - –æ—Ç–¥–µ–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞"""
    start_time = time.time()
    logger.info(f"üîß Generating E2E tests for project {project_id}")

    try:
        # –ò–º–∏—Ç–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ E2E —Ç–µ—Å—Ç–æ–≤ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É)
        self.update_state(state='PROGRESS', meta={'status': 'generating_e2e_tests'})
        await asyncio.sleep(7)

        execution_time = time.time() - start_time

        return {
            "status": "success",
            "test_type": "e2e",
            "project_id": project_id,
            "generated_tests": 5,
            "coverage_estimate": 15,
            "execution_time": execution_time
        }
    except Exception as e:
        logger.error(f"‚ùå E2E test generation failed: {e}")
        raise


@celery_app.task(bind=True, name="app.tasks.batch_generate_tests_task")
def batch_generate_tests_task(self, projects_config: List[dict]):
    """–ü–∞–∫–µ—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤"""
    logger.info(f"üöÄ Starting batch test generation for {len(projects_config)} projects")

    tasks = []
    for config in projects_config:
        task = parallel_test_generation_task.s(
            config['project_id'],
            config.get('test_config', {})
        )
        tasks.append(task)

    job = group(tasks)
    result = job.apply_async()

    return {
        "status": "started",
        "total_projects": len(projects_config),
        "task_group_id": result.id
    }


# =============================================================================
# –ó–ê–î–ê–ß–ò –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê –ò –£–ü–†–ê–í–õ–ï–ù–ò–Ø
# =============================================================================

@celery_app.task(bind=True, name="app.tasks.monitor_analysis_progress_task")
def monitor_analysis_progress_task(self, analysis_ids: List[int]):
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤"""
    logger.info(f"üìä Monitoring progress for {len(analysis_ids)} analyses")

    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    monitoring_tasks = []
    for analysis_id in analysis_ids:
        task = check_analysis_status_task.s(analysis_id)
        monitoring_tasks.append(task)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    job = group(monitoring_tasks)
    result = job.apply_async()

    return {
        "status": "monitoring_started",
        "monitoring_group_id": result.id,
        "analysis_ids": analysis_ids
    }


@celery_app.task(bind=True, name="app.tasks.check_analysis_status_task")
@robust_async_to_sync
async def check_analysis_status_task(self, analysis_id: int):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    async with AsyncSessionLocal() as db:
        analysis = await db.get(Analysis, analysis_id)
        if analysis:
            return {
                "analysis_id": analysis_id,
                "status": analysis.status,
                "progress": self._get_progress_from_status(analysis.status),
                "has_result": analysis.result is not None,
                "coverage_estimate": analysis.result.get('coverage_estimate', 0) if analysis.result else 0
            }
        return {"analysis_id": analysis_id, "status": "not_found"}


def _get_progress_from_status(self, status: str) -> int:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç–∞—Ç—É—Å –≤ –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
    progress_map = {
        "pending": 0,
        "cloning": 25,
        "analyzing": 50,
        "generating": 75,
        "completed": 100,
        "failed": 0
    }
    return progress_map.get(status, 0)


@celery_app.task(bind=True, name="app.tasks.get_task_group_status_task")
def get_task_group_status_task(self, group_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≥—Ä—É–ø–ø—ã –∑–∞–¥–∞—á"""
    try:
        from celery.result import GroupResult

        group_result = GroupResult.restore(group_id, app=celery_app)

        if group_result:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
            completed = group_result.completed_count()
            total = len(group_result)

            return {
                "group_id": group_id,
                "total_tasks": total,
                "completed_tasks": completed,
                "failed_tasks": group_result.failed_count(),
                "progress_percentage": int((completed / total) * 100) if total > 0 else 0,
                "ready": group_result.ready(),
                "successful": group_result.successful(),
                "results": group_result.results if group_result.ready() else None
            }
        else:
            return {"error": "Group not found", "group_id": group_id}

    except Exception as e:
        logger.error(f"Error getting group status: {e}")
        return {"error": str(e), "group_id": group_id}


# =============================================================================
# –°–õ–£–ñ–ï–ë–ù–´–ï –ò –¢–ï–°–¢–û–í–´–ï –ó–ê–î–ê–ß–ò
# =============================================================================

@celery_app.task(bind=True, name="app.tasks.test_dependency_filtering_task")
@robust_async_to_sync
async def test_dependency_filtering_task(self, repo_url: str, branch: str = "main"):
    """–¢–µ—Å—Ç–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –í–°–ï–• –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ä–µ–¥—É
    if os.getenv('ENVIRONMENT') not in ['testing', 'development']:
        logger.warning("Dependency filtering test should run only in test/development environment")
        return {"status": "skipped", "reason": "not_test_environment"}

    logger.info(f"üß™ Testing dependency filtering with: {repo_url}, branch: {branch}")

    try:
        git_service = GitService()
        repo_path = await git_service.clone_repository(repo_url, branch)

        code_analyzer = CodeAnalyzer()
        analysis_result = await code_analyzer.analyze_repository(repo_path)

        # –õ–æ–≥–∏—Ä—É–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –î–û —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        logger.info(f"üß™ BEFORE DEPENDENCY FILTERING:")
        logger.info(f"üß™ Total files found: {analysis_result['metrics']['total_files']}")
        logger.info(f"üß™ Ignored dependency files: {analysis_result['metrics']['ignored_files']}")
        logger.info(f"üß™ Dependency files count: {analysis_result['metrics']['dependency_files_count']}")

        test_dirs_before = analysis_result['test_analysis']['test_directories']
        dependency_dirs_before = [
            d for d in test_dirs_before
            if any(dep in d.lower() for dep in [
                'node_modules', 'vendor', 'bower_components', '__pycache__',
                '.venv', 'venv', '.gradle', '.yarn'
            ])
        ]
        logger.info(f"üß™ Dependency test dirs before: {len(dependency_dirs_before)}")

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –í–°–ï–• –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        analysis_result = _filter_dependencies_from_results(analysis_result)

        # –õ–æ–≥–∏—Ä—É–µ–º –ü–û–°–õ–ï —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        logger.info(f"üß™ AFTER DEPENDENCY FILTERING:")
        logger.info(f"üß™ Clean project files: {analysis_result['metrics']['total_files']}")
        logger.info(f"üß™ Clean test files: {analysis_result['metrics']['test_files']}")

        test_dirs_after = analysis_result['test_analysis']['test_directories']
        dependency_dirs_after = [
            d for d in test_dirs_after
            if any(dep in d.lower() for dep in [
                'node_modules', 'vendor', 'bower_components', '__pycache__',
                '.venv', 'venv', '.gradle', '.yarn'
            ])
        ]
        logger.info(f"üß™ Dependency test dirs after: {len(dependency_dirs_after)}")

        if dependency_dirs_after:
            logger.error(f"‚ùå DEPENDENCIES STILL FOUND: {dependency_dirs_after}")
            return {
                "status": "error",
                "message": "Dependencies still present in results",
                "remaining_dependencies": dependency_dirs_after
            }
        else:
            logger.info("‚úÖ SUCCESS: All dependencies filtered out!")
            return {
                "status": "success",
                "message": "All dependencies successfully filtered",
                "before_filtering": {
                    "total_files": analysis_result['metrics']['total_files'],
                    "ignored_files": analysis_result['metrics']['ignored_files'],
                    "dependency_test_dirs": len(dependency_dirs_before)
                },
                "after_filtering": {
                    "project_files": analysis_result['metrics']['total_files'],
                    "test_files": analysis_result['metrics']['test_files'],
                    "dependency_test_dirs": len(dependency_dirs_after)
                }
            }

    except Exception as e:
        logger.error(f"‚ùå Dependency filtering test failed: {e}")
        raise
    finally:
        if 'repo_path' in locals():
            git_service.cleanup(repo_path)


@celery_app.task(bind=True, name="app.tasks.diagnostic_task")
@robust_async_to_sync
async def diagnostic_task(self, test_type: str = "basic"):
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏"""
    logger.info(f"üîß Starting diagnostic task: {test_type}")

    try:
        if test_type == "basic":
            logger.info("üîÑ Testing asyncio.sleep...")
            await asyncio.sleep(2)
            logger.info("‚úÖ asyncio.sleep completed")
            return {"status": "success", "test": "basic_async"}

        elif test_type == "db":
            logger.info("üîÑ Testing database connection...")
            async with AsyncSessionLocal() as db:
                result = await db.execute("SELECT 1")
                value = result.scalar()
                logger.info(f"‚úÖ Database test completed: {value}")
            return {"status": "success", "test": "database"}

        elif test_type == "git":
            logger.info("üîÑ Testing Git operations...")
            git_service = GitService()
            repo_path = await git_service.clone_repository(
                "https://github.com/octocat/Hello-World",
                "main"
            )
            logger.info(f"‚úÖ Git clone completed: {repo_path}")
            git_service.cleanup(repo_path)
            return {"status": "success", "test": "git"}

        elif test_type == "parallel":
            logger.info("üîÑ Testing parallel execution...")
            # –°–æ–∑–¥–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            tasks = []
            for i in range(3):
                task = diagnostic_parallel_subtask.s(i)
                tasks.append(task)

            job = group(tasks)
            result = job.apply_async()

            return {
                "status": "parallel_test_started",
                "task_group_id": result.id,
                "subtask_count": 3
            }

    except Exception as e:
        logger.error(f"‚ùå Diagnostic task failed: {e}", exc_info=True)
        return {"status": "error", "test": test_type, "error": str(e)}


@celery_app.task(bind=True, name="app.tasks.diagnostic_parallel_subtask")
@robust_async_to_sync
async def diagnostic_parallel_subtask(self, task_id: int):
    """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞"""
    logger.info(f"üîß Starting parallel subtask {task_id}")
    await asyncio.sleep(2)  # –ò–º–∏—Ç–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã
    return {"status": "success", "task_id": task_id, "result": f"subtask_{task_id}_completed"}


@celery_app.task(bind=True, name="app.tasks.cleanup_old_analyses_task")
@robust_async_to_sync
async def cleanup_old_analyses_task(self, days_old: int = 30):
    """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤"""
    try:
        from datetime import timedelta
        from sqlalchemy import delete

        cutoff_date = datetime.utcnow() - timedelta(days=days_old)

        async with AsyncSessionLocal() as db:
            # –ù–∞—Ö–æ–¥–∏–º –∞–Ω–∞–ª–∏–∑—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
            result = await db.execute(
                select(Analysis).where(Analysis.created_at < cutoff_date)
            )
            old_analyses = result.scalars().all()

            # –£–¥–∞–ª—è–µ–º –∏—Ö
            deleted_count = 0
            for analysis in old_analyses:
                await db.delete(analysis)
                deleted_count += 1

            await db.commit()

            logger.info(f"üßπ Cleaned up {deleted_count} analyses older than {days_old} days")
            return {
                "status": "success",
                "deleted_count": deleted_count,
                "cutoff_date": cutoff_date.isoformat()
            }

    except Exception as e:
        logger.error(f"‚ùå Cleanup task failed: {e}")
        return {"status": "error", "error": str(e)}


def _calculate_real_coverage(analysis_result):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç coverage –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ —Ç–µ—Å—Ç–∞—Ö"""
    if not analysis_result['test_analysis']['has_tests']:
        return 0

    total_files = analysis_result['metrics']['code_files']
    test_files = analysis_result['metrics']['test_files']

    if total_files == 0:
        return 0

    # –ë–∞–∑–æ–≤—ã–π —Ä–∞—Å—á–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∫ –æ–±—â–∏–º
    file_coverage_ratio = test_files / total_files

    # –£—á–∏—Ç—ã–≤–∞–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤
    framework_bonus = len(analysis_result['test_analysis']['test_frameworks']) * 5

    # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–µ—Å—Ç–æ–≤)
    structure_bonus = len(analysis_result['test_analysis']['test_directories']) * 3

    coverage = min(85, int(file_coverage_ratio * 70 + framework_bonus + structure_bonus))

    return max(10, coverage)  # –ú–∏–Ω–∏–º—É–º 10% –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ—Å—Ç—ã