import asyncio
import os
import tempfile
import zipfile
from datetime import datetime
from celery import current_task
from app.celery_app import celery_app
from app.db.session import AsyncSessionLocal
from app.models import Analysis, Project
from app.services.git_service import GitService
from app.services.code_analyzer import CodeAnalyzer
from app.utils.async_utils import robust_async_to_sync
import logging
import shutil

logger = logging.getLogger(__name__)

async def update_analysis_status(analysis_id: int, status: str, error_message: str = None):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤ –ë–î"""
    async with AsyncSessionLocal() as db:
        analysis = await db.get(Analysis, analysis_id)
        if analysis:
            analysis.status = status
            if error_message:
                analysis.error_message = error_message
            await db.commit()
            logger.info(f"‚úÖ Updated analysis {analysis_id} status to {status}")

async def update_analysis_result(analysis_id: int, status: str, result: dict):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤ –ë–î"""
    async with AsyncSessionLocal() as db:
        analysis = await db.get(Analysis, analysis_id)
        if analysis:
            analysis.status = status
            analysis.result = result
            await db.commit()
            logger.info(f"‚úÖ Analysis {analysis_id} result updated in DB")

async def get_project_info(analysis_id: int):
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    async with AsyncSessionLocal() as db:
        analysis = await db.get(Analysis, analysis_id)
        if analysis:
            project = await db.get(Project, analysis.project_id)
            if project:
                return (project.id, project.repo_url, project.branch)
    return None

async def perform_repository_analysis(analysis_id: int):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è"""
    try:
        logger.info(f"üîç Starting REAL repository analysis for ID: {analysis_id}")

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ –ë–î
        await update_analysis_status(analysis_id, "cloning")

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ
        project_info = await get_project_info(analysis_id)
        if not project_info:
            raise Exception("Project not found")

        project_id, repo_url, branch = project_info

        logger.info(f"üì¶ Cloning repository: {repo_url}, branch: {branch}")

        # –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
        git_service = GitService()
        repo_path = await git_service.clone_repository(repo_url, branch)

        logger.info(f"‚úÖ Repository cloned to: {repo_path}")

        try:
            await update_analysis_status(analysis_id, "analyzing")

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–¥ —Å —Ä–µ–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º
            code_analyzer = CodeAnalyzer()
            analysis_result = await code_analyzer.analyze_repository(repo_path)

            logger.info(f"üìä REAL analysis completed:")
            logger.info(f"  - Technologies: {analysis_result.get('technologies', [])}")
            logger.info(f"  - Frameworks: {analysis_result.get('frameworks', [])}")
            logger.info(f"  - Total files: {analysis_result['metrics']['total_files']}")
            logger.info(f"  - Code files: {analysis_result['metrics']['code_files']}")
            logger.info(f"  - Test files: {analysis_result['metrics']['test_files']}")

            await update_analysis_status(analysis_id, "generating")

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º coverage –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            coverage_estimate = _calculate_real_coverage(analysis_result)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¢–û–õ–¨–ö–û –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            result_data = {
                "technologies": analysis_result.get('technologies', []),
                "frameworks": analysis_result.get('frameworks', []),
                "file_structure_summary": {
                    "total_files": analysis_result['metrics']['total_files'],
                    "code_files": analysis_result['metrics']['code_files'],
                    "test_files": analysis_result['metrics']['test_files'],
                    "total_lines": analysis_result['metrics']['total_lines'],
                    "total_size_kb": round(analysis_result['metrics']['total_size_kb'], 2),
                },
                "test_analysis": {
                    "has_tests": analysis_result['test_analysis']['has_tests'],
                    "test_frameworks": analysis_result['test_analysis']['test_frameworks'],
                    "test_files_count": analysis_result['test_analysis']['test_files_count'],
                    "test_directories": analysis_result['test_analysis']['test_directories'],
                },
                "project_structure": analysis_result['project_structure'],
                "dependencies": analysis_result['dependencies'],
                "complexity_metrics": analysis_result['complexity_metrics'],
                "source": repo_url,
                "branch": branch,
                "coverage_estimate": coverage_estimate,
                "analysis_timestamp": datetime.utcnow().isoformat()
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –†–ï–ê–õ–¨–ù–´–ô —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ë–î
            await update_analysis_result(
                analysis_id,
                "completed",
                result_data
            )

            logger.info(f"‚úÖ REAL Repository analysis {analysis_id} completed successfully")
            logger.info(f"üìà Final coverage estimate: {coverage_estimate}%")

            return {
                "status": "success",
                "analysis_id": analysis_id,
                "technologies": analysis_result.get('technologies', []),
                "has_tests": analysis_result['test_analysis']['has_tests'],
                "test_frameworks": analysis_result['test_analysis']['test_frameworks'],
                "coverage": coverage_estimate
            }

        except Exception as e:
            logger.error(f"‚ùå Analysis failed: {e}")
            await update_analysis_status(analysis_id, "failed", str(e))
            raise
        finally:
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            git_service.cleanup(repo_path)

    except Exception as e:
        logger.error(f"‚ùå Repository analysis {analysis_id} failed: {str(e)}")
        await update_analysis_status(analysis_id, "failed", str(e))
        raise

async def perform_zip_analysis(analysis_id: int, zip_path: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ ZIP –∞—Ä—Ö–∏–≤–∞"""
    try:
        logger.info(f"üì¶ Starting REAL ZIP analysis for ID: {analysis_id}")

        await update_analysis_status(analysis_id, "extracting")

        # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º ZIP
        extract_path = tempfile.mkdtemp(prefix="extracted_")

        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)

        logger.info(f"‚úÖ ZIP extracted to: {extract_path}")

        try:
            await update_analysis_status(analysis_id, "analyzing")

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–¥ —Å —Ä–µ–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º
            code_analyzer = CodeAnalyzer()
            analysis_result = await code_analyzer.analyze_repository(extract_path)

            logger.info(f"üìä REAL ZIP analysis completed:")
            logger.info(f"  - Technologies: {analysis_result.get('technologies', [])}")
            logger.info(f"  - Total files: {analysis_result['metrics']['total_files']}")
            logger.info(f"  - Test files: {analysis_result['metrics']['test_files']}")

            await update_analysis_status(analysis_id, "generating")

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º coverage –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            coverage_estimate = _calculate_real_coverage(analysis_result)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            result_data = {
                "technologies": analysis_result.get('technologies', []),
                "frameworks": analysis_result.get('frameworks', []),
                "file_structure_summary": {
                    "total_files": analysis_result['metrics']['total_files'],
                    "code_files": analysis_result['metrics']['code_files'],
                    "test_files": analysis_result['metrics']['test_files'],
                    "total_lines": analysis_result['metrics']['total_lines'],
                    "total_size_kb": round(analysis_result['metrics']['total_size_kb'], 2),
                },
                "test_analysis": {
                    "has_tests": analysis_result['test_analysis']['has_tests'],
                    "test_frameworks": analysis_result['test_analysis']['test_frameworks'],
                    "test_files_count": analysis_result['test_analysis']['test_files_count'],
                    "test_directories": analysis_result['test_analysis']['test_directories'],
                },
                "project_structure": analysis_result['project_structure'],
                "dependencies": analysis_result['dependencies'],
                "complexity_metrics": analysis_result['complexity_metrics'],
                "source": "ZIP Archive",
                "branch": "main",
                "coverage_estimate": coverage_estimate,
                "analysis_timestamp": datetime.utcnow().isoformat()
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            await update_analysis_result(
                analysis_id,
                "completed",
                result_data
            )

            logger.info(f"‚úÖ ZIP analysis {analysis_id} completed")

            return {
                "status": "success",
                "analysis_id": analysis_id,
                "technologies": analysis_result.get('technologies', []),
                "has_tests": analysis_result['test_analysis']['has_tests'],
                "test_frameworks": analysis_result['test_analysis']['test_frameworks'],
                "coverage": coverage_estimate
            }

        finally:
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            shutil.rmtree(extract_path, ignore_errors=True)
            if os.path.exists(zip_path):
                os.remove(zip_path)

    except Exception as e:
        logger.error(f"‚ùå ZIP analysis {analysis_id} failed: {str(e)}")
        await update_analysis_status(analysis_id, "failed", str(e))
        raise

@celery_app.task(bind=True, name="app.tasks.analyze_repository_task")
@robust_async_to_sync
async def analyze_repository_task(self, analysis_id: int):
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –∏–∑ GitHub"""
    logger.info(f"üéØ Starting analyze_repository_task for analysis_id: {analysis_id}")
    try:
        result = await perform_repository_analysis(analysis_id)
        logger.info(f"‚úÖ analyze_repository_task completed for analysis_id: {analysis_id}")
        return result
    except Exception as e:
        logger.error(f"‚ùå analyze_repository_task failed for analysis_id {analysis_id}: {str(e)}")
        return {
            "status": "error",
            "analysis_id": analysis_id,
            "error": str(e)
        }

@celery_app.task(bind=True, name="app.tasks.analyze_zip_task")
@robust_async_to_sync
async def analyze_zip_task(self, analysis_id: int, zip_path: str):
    """–ê–Ω–∞–ª–∏–∑ ZIP –∞—Ä—Ö–∏–≤–∞"""
    logger.info(f"üéØ Starting analyze_zip_task for analysis_id: {analysis_id}")
    try:
        result = await perform_zip_analysis(analysis_id, zip_path)
        logger.info(f"‚úÖ analyze_zip_task completed for analysis_id: {analysis_id}")
        return result
    except Exception as e:
        logger.error(f"‚ùå analyze_zip_task failed for analysis_id {analysis_id}: {str(e)}")
        return {
            "status": "error",
            "analysis_id": analysis_id,
            "error": str(e)
        }


@celery_app.task(bind=True, name="app.tasks.diagnostic_task")
@robust_async_to_sync
async def diagnostic_task(self, test_type: str = "basic"):
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏"""
    logger.info(f"üîß Starting diagnostic task: {test_type}")

    try:
        if test_type == "basic":
            # –¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏
            logger.info("üîÑ Testing asyncio.sleep...")
            await asyncio.sleep(2)
            logger.info("‚úÖ asyncio.sleep completed")

            return {"status": "success", "test": "basic_async"}

        elif test_type == "db":
            # –¢–µ—Å—Ç —Ä–∞–±–æ—Ç—ã —Å –ë–î
            logger.info("üîÑ Testing database connection...")
            async with AsyncSessionLocal() as db:
                result = await db.execute("SELECT 1")
                value = result.scalar()
                logger.info(f"‚úÖ Database test completed: {value}")

            return {"status": "success", "test": "database"}

        elif test_type == "git":
            # –¢–µ—Å—Ç Git –æ–ø–µ—Ä–∞—Ü–∏–π
            logger.info("üîÑ Testing Git operations...")
            git_service = GitService()
            repo_path = await git_service.clone_repository(
                "https://github.com/octocat/Hello-World",
                "main"
            )
            logger.info(f"‚úÖ Git clone completed: {repo_path}")

            # –û—á–∏—Å—Ç–∫–∞
            git_service.cleanup(repo_path)
            return {"status": "success", "test": "git"}

    except Exception as e:
        logger.error(f"‚ùå Diagnostic task failed: {e}", exc_info=True)
        return {"status": "error", "test": test_type, "error": str(e)}

def _calculate_real_coverage(analysis_result):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç coverage –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ —Ç–µ—Å—Ç–∞—Ö"""
    if not analysis_result['test_analysis']['has_tests']:
        return 0

    total_files = analysis_result['metrics']['code_files']
    test_files = analysis_result['metrics']['test_files']

    if total_files == 0:
        return 0

    # –ë–∞–∑–æ–≤—ã–π —Ä–∞—Å—á–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∫ –æ–±—â–∏–º
    file_coverage_ratio = test_files / total_files

    # –£—á–∏—Ç—ã–≤–∞–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤
    framework_bonus = len(analysis_result['test_analysis']['test_frameworks']) * 5

    # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–µ—Å—Ç–æ–≤)
    structure_bonus = len(analysis_result['test_analysis']['test_directories']) * 3

    coverage = min(85, int(file_coverage_ratio * 70 + framework_bonus + structure_bonus))

    return max(10, coverage)  # –ú–∏–Ω–∏–º—É–º 10% –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ—Å—Ç—ã